{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae78b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 28 15:06:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c2174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"scikit-learn<1.6.0\" underthesea rouge-score pycocoevalcap vncorenlp timm transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db8162",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4991c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# Metrics\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca74cc",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0f0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    # ==== PATH DATA GỐC ====\n",
    "    TRAIN_IMAGES_PATH = '/kaggle/input/d/mrworldzero/openvivqa/data/train-images/training-images'\n",
    "    TEST_IMAGES_PATH  = '/kaggle/input/d/mrworldzero/openvivqa/data/test-images/test-images'\n",
    "    DEV_IMAGES_PATH   = '/kaggle/input/d/mrworldzero/openvivqa/data/dev-images/dev-images'\n",
    "    \n",
    "    TRAIN_JSON_PATH   = '/kaggle/input/d/mrworldzero/openvivqa/data/vlsp2023_train_data.json'\n",
    "    TEST_JSON_PATH    = '/kaggle/input/d/mrworldzero/openvivqa/data/vlsp2023_test_data.json'\n",
    "    DEV_JSON_PATH     = '/kaggle/input/d/mrworldzero/openvivqa/data/vlsp2023_dev_data.json'\n",
    "    \n",
    "    # ==== JSON FLAT SAU KHI CHUYỂN ====\n",
    "    TRAIN_JSON_FLAT = \"/kaggle/working/data/train_flat.json\"\n",
    "    DEV_JSON_FLAT   = \"/kaggle/working/data/val_flat.json\"\n",
    "    TEST_JSON_FLAT  = \"/kaggle/working/data/test_flat.json\"\n",
    "    \n",
    "    # ==== TRAINING ====\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = 224\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    \n",
    "    LR_VIT_PHOBERT = 2e-5      # LR cho encoder (ViT + PhoBERT)\n",
    "    LR_DECODER = 5e-4          # LR cho decoder + fusion\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ==== MÔ HÌNH ====\n",
    "    TEXT_ENCODER_NAME = \"vinai/phobert-base-v2\"\n",
    "    VISION_NAME = \"vit_base_patch16_224\"\n",
    "    \n",
    "    MAX_QUESTION_LEN = 64\n",
    "    MAX_ANSWER_LEN = 15\n",
    "    DEC_HIDDEN_SIZE = 256\n",
    "    MIN_ANSWER_FREQ = 3  # min freq để đưa token vào vocab\n",
    "    \n",
    "    NUM_BEAMS = 5  # ở đây mình dùng greedy; nếu muốn beam search thì code thêm\n",
    "    \n",
    "    OUT_DIR = \"outputs_vqa\"\n",
    "    NUM_VIZ_EXAMPLES = 5\n",
    "\n",
    "cfg = config()\n",
    "os.makedirs(cfg.OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.SEED)\n",
    "print(f\"Using device: {cfg.DEVICE}\")\n",
    "\n",
    "# NLTK & ROUGE\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1510e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize_simple(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def vi_seg(s: str) -> List[str]:\n",
    "    \"\"\"Segment tiếng Việt bằng underthesea → list token.\"\"\"\n",
    "    s = text_normalize_simple(s)\n",
    "    return word_tokenize(s, format=\"text\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a90663",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8fafb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Saved 30833 valid items to /kaggle/working/data/train_flat.json\n",
      "[dev] Saved 3545 valid items to /kaggle/working/data/val_flat.json\n",
      "[test] Saved 14035 valid items to /kaggle/working/data/test_flat.json\n"
     ]
    }
   ],
   "source": [
    "def convert_json_to_flat_json(input_file, folder_path, output_file, split=\"train\"):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    flat_list = []\n",
    "    skipped = 0\n",
    "\n",
    "    for anno_id, annotation in data[\"annotations\"].items():\n",
    "        image_id = annotation[\"image_id\"]\n",
    "        question = annotation[\"question\"]\n",
    "\n",
    "        image_name = data[\"images\"].get(str(image_id), \"\")\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        if split in [\"train\", \"dev\"]:\n",
    "            answer = str(annotation.get(\"answer\", \"\")).strip()\n",
    "\n",
    "            if (answer == \"\") or (answer.lower() == \"your answer\"):\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            flat_item = {\n",
    "                \"image_path\": image_path,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "        else:\n",
    "            flat_item = {\n",
    "                \"image_path\": image_path,\n",
    "                \"question\": question,\n",
    "            }\n",
    "\n",
    "        flat_list.append(flat_item)\n",
    "\n",
    "    out_dir = os.path.dirname(output_file)\n",
    "    if out_dir != \"\":\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flat_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[{split}] Saved {len(flat_list)} valid items to {output_file}\")\n",
    "    if split in [\"train\", \"dev\"] and skipped > 0:\n",
    "        print(f\"[{split}] Skipped {skipped} invalid items (empty or 'your answer')\")\n",
    "\n",
    "# Chạy nếu chưa có file flat\n",
    "convert_json_to_flat_json(\n",
    "    cfg.TRAIN_JSON_PATH,\n",
    "    cfg.TRAIN_IMAGES_PATH,\n",
    "    cfg.TRAIN_JSON_FLAT,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "convert_json_to_flat_json(\n",
    "    cfg.DEV_JSON_PATH,\n",
    "    cfg.DEV_IMAGES_PATH,\n",
    "    cfg.DEV_JSON_FLAT,\n",
    "    split=\"dev\"\n",
    ")\n",
    "\n",
    "convert_json_to_flat_json(\n",
    "    cfg.TEST_JSON_PATH,\n",
    "    cfg.TEST_IMAGES_PATH,\n",
    "    cfg.TEST_JSON_FLAT,\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1b390",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100198ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (answer): 3498 (min_freq=3)\n"
     ]
    }
   ],
   "source": [
    "def build_answer_vocab(train_json_flat: str, min_freq: int = 1):\n",
    "    with open(train_json_flat, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    counter = Counter()\n",
    "    for item in data:\n",
    "        ans = item[\"answer\"]\n",
    "        tokens = vi_seg(ans)\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # SPECIAL\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    stoi = {}\n",
    "    itos = []\n",
    "    \n",
    "    for sp in specials:\n",
    "        idx = len(itos)\n",
    "        itos.append(sp)\n",
    "        stoi[sp] = idx\n",
    "    \n",
    "    for tok, freq in counter.items():\n",
    "        if freq >= min_freq and tok not in stoi:\n",
    "            idx = len(itos)\n",
    "            itos.append(tok)\n",
    "            stoi[tok] = idx\n",
    "    \n",
    "    print(f\"Vocab size (answer): {len(itos)} (min_freq={min_freq})\")\n",
    "    return stoi, itos\n",
    "\n",
    "answer_stoi, answer_itos = build_answer_vocab(cfg.TRAIN_JSON_FLAT, cfg.MIN_ANSWER_FREQ)\n",
    "\n",
    "PAD_ID = answer_stoi[\"<pad>\"]\n",
    "BOS_ID = answer_stoi[\"<bos>\"]\n",
    "EOS_ID = answer_stoi[\"<eos>\"]\n",
    "UNK_ID = answer_stoi[\"<unk>\"]\n",
    "\n",
    "def encode_answer(text: str) -> torch.Tensor:\n",
    "    tokens = vi_seg(text)\n",
    "    ids = [BOS_ID] + [answer_stoi.get(t, UNK_ID) for t in tokens] + [EOS_ID]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def decode_answer_ids(ids: List[int]) -> str:\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        if i == EOS_ID:\n",
    "            break\n",
    "        if i in [PAD_ID, BOS_ID]:\n",
    "            continue\n",
    "        if 0 <= i < len(answer_itos):\n",
    "            tokens.append(answer_itos[i])\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2314f6",
   "metadata": {},
   "source": [
    "# Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcced36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30833 3545 14035\n"
     ]
    }
   ],
   "source": [
    "# PhoBERT tokenizer cho câu hỏi\n",
    "q_tokenizer = AutoTokenizer.from_pretrained(cfg.TEXT_ENCODER_NAME)\n",
    "\n",
    "# Image transform cho ViT\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n",
    "    transforms.RandomResizedCrop(cfg.IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.1, contrast=0.1, saturation=0.1\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, json_flat_path: str, tokenizer, transform, has_answer: bool = True):\n",
    "        super().__init__()\n",
    "        with open(json_flat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.has_answer = has_answer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_path = item[\"image_path\"]\n",
    "        question = item[\"question\"]\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Encode question bằng PhoBERT tokenizer\n",
    "        encoded = self.tokenizer(\n",
    "            text_normalize_simple(question),\n",
    "            max_length=cfg.MAX_QUESTION_LEN,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        q_input_ids = encoded[\"input_ids\"].squeeze(0)      # (L,)\n",
    "        q_attention = encoded[\"attention_mask\"].squeeze(0) # (L,)\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": img,\n",
    "            \"question\": question,\n",
    "            \"q_input_ids\": q_input_ids,\n",
    "            \"q_attention_mask\": q_attention\n",
    "        }\n",
    "        \n",
    "        if self.has_answer:\n",
    "            answer = item[\"answer\"]\n",
    "            ans_ids = encode_answer(answer)\n",
    "            sample[\"answer\"] = answer\n",
    "            sample[\"answer_ids\"] = ans_ids\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def vqa_collate_fn(batch):\n",
    "    images = torch.stack([b[\"image\"] for b in batch], dim=0)\n",
    "    q_input_ids = torch.stack([b[\"q_input_ids\"] for b in batch], dim=0)\n",
    "    q_attention = torch.stack([b[\"q_attention_mask\"] for b in batch], dim=0)\n",
    "    \n",
    "    out = {\n",
    "        \"images\": images,\n",
    "        \"q_input_ids\": q_input_ids,\n",
    "        \"q_attention_mask\": q_attention,\n",
    "        \"questions\": [b[\"question\"] for b in batch],\n",
    "    }\n",
    "    \n",
    "    if \"answer_ids\" in batch[0]:\n",
    "        ans_seqs = [b[\"answer_ids\"] for b in batch]\n",
    "        ans_padded = pad_sequence(ans_seqs, batch_first=True, padding_value=PAD_ID)\n",
    "        out[\"answer_ids\"] = ans_padded\n",
    "        out[\"answers\"] = [b[\"answer\"] for b in batch]\n",
    "    \n",
    "    return out\n",
    "\n",
    "train_dataset = VQADataset(cfg.TRAIN_JSON_FLAT, q_tokenizer, image_transform, has_answer=True)\n",
    "dev_dataset   = VQADataset(cfg.DEV_JSON_FLAT,   q_tokenizer, image_transform, has_answer=True)\n",
    "test_dataset  = VQADataset(cfg.TEST_JSON_FLAT,  q_tokenizer, image_transform, has_answer=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.NUM_WORKERS,\n",
    "    collate_fn=vqa_collate_fn\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS,\n",
    "    collate_fn=vqa_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS,\n",
    "    collate_fn=vqa_collate_fn\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(dev_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31402e84",
   "metadata": {},
   "source": [
    "# Encoder ViT + PhoBERT và Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394fa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Giả sử đã có PAD_ID, BOS_ID, EOS_ID, decode_answer_ids, cfg.DEC_HIDDEN_SIZE trong môi trường.\n",
    "\n",
    "\n",
    "class ViTEncoderTokens(nn.Module):\n",
    "    def __init__(self, model_name: str = \"vit_base_patch16_224\"):\n",
    "        super().__init__()\n",
    "        # global_pool=\"\" để trả ra chuỗi token patch\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\"\n",
    "        )\n",
    "        self.out_dim = self.backbone.num_features  # D\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B,3,H,W)\n",
    "        return: (B, N_img, D)\n",
    "        \"\"\"\n",
    "        feats = self.backbone(x)  # (B, N_img, D)\n",
    "        return feats\n",
    "\n",
    "\n",
    "class PhoBERTEncoderSeq(nn.Module):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        return: last_hidden_state (B, L, H)\n",
    "        \"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
    "        return last_hidden\n",
    "\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, cfg, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Encoders\n",
    "        self.vit = ViTEncoderTokens(cfg.VISION_NAME)\n",
    "        self.text_encoder = PhoBERTEncoderSeq(cfg.TEXT_ENCODER_NAME)\n",
    "\n",
    "        ctx_dim = cfg.DEC_HIDDEN_SIZE\n",
    "        self.ctx_dim = ctx_dim\n",
    "\n",
    "        # Project image & text tokens về cùng chiều\n",
    "        self.img_proj = nn.Linear(self.vit.out_dim, ctx_dim)\n",
    "        self.txt_proj = nn.Linear(self.text_encoder.hidden_size, ctx_dim)\n",
    "\n",
    "        # Fusion global context (img_ctx + txt_mean) để init decoder\n",
    "        self.global_fuse = nn.Linear(ctx_dim * 2, ctx_dim)\n",
    "        self.global_ln = nn.LayerNorm(ctx_dim)\n",
    "        self.global_dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Decoder LSTM (input = [token_emb, img_ctx])\n",
    "        self.embed_ans = nn.Embedding(vocab_size, ctx_dim, padding_idx=PAD_ID)\n",
    "        self.emb_dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=ctx_dim * 2,   # token_emb + img_ctx\n",
    "            hidden_size=ctx_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Cross-attn lên chuỗi PhoBERT (text memory)\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=ctx_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Gating cho ảnh + fusion output\n",
    "        self.gate_ff = nn.Linear(ctx_dim * 3, ctx_dim)   # [h_t, text_ctx, img_ctx]\n",
    "        self.fusion_ff = nn.Linear(ctx_dim * 3, ctx_dim)\n",
    "        self.fusion_dropout = nn.Dropout(0.3)\n",
    "        self.fusion_ln = nn.LayerNorm(ctx_dim)\n",
    "\n",
    "        # Weight tying: out_proj.weight = embed_ans.weight\n",
    "        self.out_proj = nn.Linear(ctx_dim, vocab_size, bias=False)\n",
    "        self.out_proj.weight = self.embed_ans.weight\n",
    "\n",
    "    # ====== BUILD MEMORY (text) + GLOBAL CONTEXT ======\n",
    "    def build_memory(self, images, q_input_ids, q_attention_mask):\n",
    "        \"\"\"\n",
    "        images: (B,3,H,W)\n",
    "        q_input_ids: (B,L)\n",
    "        q_attention_mask: (B,L)\n",
    "\n",
    "        return:\n",
    "            img_ctx:     (B, C)\n",
    "            memory:      (B, L_txt, C)  (PhoBERT tokens)\n",
    "            memory_mask: (B, L_txt)     (1=valid, 0=pad)\n",
    "            global_ctx:  (B, C)\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "        B = images.size(0)\n",
    "\n",
    "        # Image tokens -> mean pool\n",
    "        img_tokens = self.vit(images)                  # (B, N_img, D_v)\n",
    "        img_feats = torch.tanh(self.img_proj(img_tokens))  # (B, N_img, C)\n",
    "        img_ctx = img_feats.mean(dim=1)                # (B, C)\n",
    "\n",
    "        # Text tokens\n",
    "        txt_tokens = self.text_encoder(q_input_ids, q_attention_mask)  # (B,L,H_t)\n",
    "        txt_feats = torch.tanh(self.txt_proj(txt_tokens))              # (B,L,C)\n",
    "        txt_mask = q_attention_mask                                    # (B,L)\n",
    "\n",
    "        # mean pooling text để tạo txt_ctx\n",
    "        mask_exp = txt_mask.unsqueeze(-1)                      # (B,L,1)\n",
    "        txt_sum = (txt_feats * mask_exp).sum(dim=1)            # (B,C)\n",
    "        lengths = mask_exp.sum(dim=1).clamp(min=1)             # (B,1)\n",
    "        txt_ctx = txt_sum / lengths                            # (B,C)\n",
    "\n",
    "        # global context = fusion(img_ctx, txt_ctx)\n",
    "        global_ctx = torch.tanh(self.global_fuse(\n",
    "            torch.cat([img_ctx, txt_ctx], dim=-1)\n",
    "        ))                                                     # (B,C)\n",
    "        global_ctx = self.global_ln(global_ctx)\n",
    "        global_ctx = self.global_dropout(global_ctx)\n",
    "\n",
    "        memory = txt_feats                                     # (B,L,C)\n",
    "        memory_mask = txt_mask                                 # (B,L)\n",
    "\n",
    "        return img_ctx, memory, memory_mask, global_ctx\n",
    "\n",
    "    # ====== TRAIN FORWARD ======\n",
    "    def forward(self, images, q_input_ids, q_attention_mask, ans_input_ids):\n",
    "        \"\"\"\n",
    "        ans_input_ids: (B, L_ans) với [BOS, ..., EOS]\n",
    "        \"\"\"\n",
    "        img_ctx, memory, memory_mask, global_ctx = self.build_memory(\n",
    "            images, q_input_ids, q_attention_mask\n",
    "        )  # img_ctx:(B,C), memory:(B,Lm,C), mask:(B,Lm), global_ctx:(B,C)\n",
    "\n",
    "        # Decoder inputs\n",
    "        dec_in_ids = ans_input_ids[:, :-1]  # (B, L-1)\n",
    "        targets    = ans_input_ids[:, 1:]   # (B, L-1)\n",
    "\n",
    "        tok_emb = self.embed_ans(dec_in_ids)   # (B, L-1, C)\n",
    "        tok_emb = self.emb_dropout(tok_emb)\n",
    "\n",
    "        # lặp img_ctx theo thời gian\n",
    "        img_ctx_exp = img_ctx.unsqueeze(1).expand(-1, tok_emb.size(1), -1)  # (B,L-1,C)\n",
    "        dec_input = torch.cat([tok_emb, img_ctx_exp], dim=-1)               # (B,L-1,2C)\n",
    "\n",
    "        # Init hidden/cell\n",
    "        h0 = global_ctx.unsqueeze(0)           # (1,B,C)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        dec_out, (h_n, c_n) = self.decoder(dec_input, (h0, c0))   # (B,L-1,C)\n",
    "\n",
    "        # Cross-attention lên text memory\n",
    "        attn_ctx, _ = self.cross_attn(\n",
    "            dec_out,              # query\n",
    "            memory, memory,       # key, value\n",
    "            key_padding_mask=(memory_mask == 0)\n",
    "        )  # (B,L-1,C)\n",
    "\n",
    "        # Gating với ảnh\n",
    "        img_ctx_time = img_ctx.unsqueeze(1).expand_as(dec_out)  # (B,L-1,C)\n",
    "        gate_input = torch.cat([dec_out, attn_ctx, img_ctx_time], dim=-1)  # (B,L-1,3C)\n",
    "        gate = torch.sigmoid(self.gate_ff(gate_input))                      # (B,L-1,C)\n",
    "        visual_ctx = gate * img_ctx_time                                    # (B,L-1,C)\n",
    "\n",
    "        fused = torch.cat([dec_out, attn_ctx, visual_ctx], dim=-1)          # (B,L-1,3C)\n",
    "        fused = torch.tanh(self.fusion_ff(fused))                           # (B,L-1,C)\n",
    "        fused = self.fusion_dropout(fused)\n",
    "        fused = self.fusion_ln(fused)\n",
    "\n",
    "        logits = self.out_proj(fused)                                       # (B,L-1,V)\n",
    "        return logits, targets\n",
    "\n",
    "    # ====== 1 STEP DECODE (cho greedy / beam) ======\n",
    "    def _decode_step(self, prev_token, h, c, memory, memory_mask, img_ctx):\n",
    "        \"\"\"\n",
    "        prev_token: (B,)\n",
    "        h, c:      (1,B,C)\n",
    "        memory:    (B,Lm,C)\n",
    "        memory_mask:(B,Lm)\n",
    "        img_ctx:   (B,C)\n",
    "        \"\"\"\n",
    "        tok_emb = self.embed_ans(prev_token).unsqueeze(1)  # (B,1,C)\n",
    "        tok_emb = self.emb_dropout(tok_emb)\n",
    "\n",
    "        img_ctx_exp = img_ctx.unsqueeze(1)                 # (B,1,C)\n",
    "        dec_input = torch.cat([tok_emb, img_ctx_exp], dim=-1)  # (B,1,2C)\n",
    "\n",
    "        dec_out, (h, c) = self.decoder(dec_input, (h, c))      # (B,1,C)\n",
    "\n",
    "        attn_ctx, _ = self.cross_attn(\n",
    "            dec_out, memory, memory,\n",
    "            key_padding_mask=(memory_mask == 0)\n",
    "        )  # (B,1,C)\n",
    "\n",
    "        gate_input = torch.cat([dec_out, attn_ctx, img_ctx_exp], dim=-1)    # (B,1,3C)\n",
    "        gate = torch.sigmoid(self.gate_ff(gate_input))                       # (B,1,C)\n",
    "        visual_ctx = gate * img_ctx_exp                                     # (B,1,C)\n",
    "\n",
    "        fused = torch.cat([dec_out, attn_ctx, visual_ctx], dim=-1)          # (B,1,3C)\n",
    "        fused = torch.tanh(self.fusion_ff(fused))                           # (B,1,C)\n",
    "        fused = self.fusion_dropout(fused)\n",
    "        fused = self.fusion_ln(fused)\n",
    "\n",
    "        logits = self.out_proj(fused.squeeze(1))                            # (B,V)\n",
    "        return logits, h, c\n",
    "\n",
    "    # ====== GREEDY DECODE ======\n",
    "    def generate_greedy(self, images, q_input_ids, q_attention_mask,\n",
    "                        max_len=32, bos_id=BOS_ID, eos_id=EOS_ID):\n",
    "        self.eval()\n",
    "        decoded = []\n",
    "        with torch.no_grad():\n",
    "            img_ctx, memory, memory_mask, global_ctx = self.build_memory(\n",
    "                images, q_input_ids, q_attention_mask\n",
    "            )  # img_ctx:(B,C), memory:(B,Lm,C), mask:(B,Lm), global:(B,C)\n",
    "            B = images.size(0)\n",
    "\n",
    "            h = global_ctx.unsqueeze(0)              # (1,B,C)\n",
    "            c = torch.zeros_like(h)\n",
    "            prev_tokens = torch.full(\n",
    "                (B,), bos_id, dtype=torch.long, device=images.device\n",
    "            )\n",
    "            finished = torch.zeros(B, dtype=torch.bool, device=images.device)\n",
    "            sequences = [[] for _ in range(B)]\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                logits, h, c = self._decode_step(\n",
    "                    prev_tokens, h, c, memory, memory_mask, img_ctx\n",
    "                )\n",
    "                next_tokens = logits.argmax(dim=-1)   # (B,)\n",
    "\n",
    "                for i in range(B):\n",
    "                    if finished[i]:\n",
    "                        continue\n",
    "                    tid = next_tokens[i].item()\n",
    "                    if tid == eos_id:\n",
    "                        finished[i] = True\n",
    "                    else:\n",
    "                        sequences[i].append(tid)\n",
    "\n",
    "                if finished.all():\n",
    "                    break\n",
    "\n",
    "                prev_tokens = next_tokens\n",
    "\n",
    "            decoded = [decode_answer_ids(seq) for seq in sequences]\n",
    "        return decoded\n",
    "\n",
    "    # ====== BEAM SEARCH CHO 1 SAMPLE ======\n",
    "    def _generate_one_beam(self, img_ctx, memory, memory_mask, global_ctx,\n",
    "                           max_len=32, bos_id=BOS_ID, eos_id=EOS_ID, num_beams=3):\n",
    "        \"\"\"\n",
    "        img_ctx:    (1,C)\n",
    "        memory:     (1,Lm,C)\n",
    "        memory_mask:(1,Lm)\n",
    "        global_ctx: (1,C)\n",
    "        \"\"\"\n",
    "        device = img_ctx.device\n",
    "        C = img_ctx.size(-1)\n",
    "        Lm = memory.size(1)\n",
    "        beam_size = num_beams\n",
    "\n",
    "        img_beam = img_ctx.expand(beam_size, C)            # (beam,C)\n",
    "        mem_beam = memory.expand(beam_size, Lm, C)         # (beam,Lm,C)\n",
    "        mask_beam = memory_mask.expand(beam_size, Lm)      # (beam,Lm)\n",
    "        h = global_ctx.expand(1, beam_size, C).contiguous()# (1,beam,C)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        prev_tokens = torch.full(\n",
    "            (beam_size,), bos_id, dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        beams = [{\n",
    "            \"tokens\": [],\n",
    "            \"log_prob\": 0.0,\n",
    "            \"finished\": False\n",
    "        }] + [{\n",
    "            \"tokens\": [],\n",
    "            \"log_prob\": float(\"-inf\"),\n",
    "            \"finished\": True\n",
    "        } for _ in range(beam_size - 1)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, h, c = self._decode_step(\n",
    "                prev_tokens, h, c, mem_beam, mask_beam, img_beam\n",
    "            )  # logits: (beam,V)\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)  # (beam,V)\n",
    "\n",
    "            # beam đã finish → chỉ cho phép EOS\n",
    "            for i, beam in enumerate(beams):\n",
    "                if beam[\"finished\"]:\n",
    "                    log_probs[i, :] = float(\"-inf\")\n",
    "                    log_probs[i, eos_id] = 0.0\n",
    "\n",
    "            beam_log_probs = torch.tensor(\n",
    "                [b[\"log_prob\"] for b in beams], device=device\n",
    "            ).unsqueeze(1)  # (beam,1)\n",
    "            total_log_probs = log_probs + beam_log_probs       # (beam,V)\n",
    "\n",
    "            flat = total_log_probs.view(-1)                    # (beam*V,)\n",
    "            topk_log_probs, topk_indices = torch.topk(flat, beam_size)\n",
    "\n",
    "            new_beams = []\n",
    "            new_prev_tokens = torch.zeros_like(prev_tokens)\n",
    "\n",
    "            V = logits.size(-1)\n",
    "            for new_i, (lp, idx) in enumerate(zip(topk_log_probs, topk_indices)):\n",
    "                beam_idx = (idx // V).item()\n",
    "                token_id = (idx % V).item()\n",
    "\n",
    "                old_beam = beams[beam_idx]\n",
    "                new_tokens = old_beam[\"tokens\"].copy()\n",
    "                finished = old_beam[\"finished\"]\n",
    "\n",
    "                if not finished:\n",
    "                    if token_id == eos_id:\n",
    "                        finished = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_id)\n",
    "\n",
    "                new_beams.append({\n",
    "                    \"tokens\": new_tokens,\n",
    "                    \"log_prob\": lp.item(),\n",
    "                    \"finished\": finished\n",
    "                })\n",
    "                new_prev_tokens[new_i] = token_id\n",
    "\n",
    "            beams = new_beams\n",
    "            prev_tokens = new_prev_tokens\n",
    "\n",
    "            if all(b[\"finished\"] for b in beams):\n",
    "                break\n",
    "\n",
    "        finished_beams = [b for b in beams if b[\"finished\"] and len(b[\"tokens\"]) > 0]\n",
    "        if len(finished_beams) == 0:\n",
    "            finished_beams = beams\n",
    "        best_beam = max(finished_beams, key=lambda b: b[\"log_prob\"])\n",
    "        return best_beam[\"tokens\"]\n",
    "\n",
    "    def generate_beam(self, images, q_input_ids, q_attention_mask,\n",
    "                      max_len=32, bos_id=BOS_ID, eos_id=EOS_ID, num_beams=None):\n",
    "        if num_beams is None:\n",
    "            num_beams = getattr(self.cfg, \"NUM_BEAMS\", 3)\n",
    "\n",
    "        self.eval()\n",
    "        decoded = []\n",
    "        with torch.no_grad():\n",
    "            img_ctx, memory, memory_mask, global_ctx = self.build_memory(\n",
    "                images, q_input_ids, q_attention_mask\n",
    "            )  # img_ctx:(B,C), memory:(B,Lm,C), mask:(B,Lm), global:(B,C)\n",
    "            B = images.size(0)\n",
    "            for i in range(B):\n",
    "                img_i = img_ctx[i:i+1]\n",
    "                mem_i = memory[i:i+1]\n",
    "                mask_i = memory_mask[i:i+1]\n",
    "                ctx_i = global_ctx[i:i+1]\n",
    "                token_ids = self._generate_one_beam(\n",
    "                    img_i, mem_i, mask_i, ctx_i,\n",
    "                    max_len=max_len,\n",
    "                    bos_id=bos_id,\n",
    "                    eos_id=eos_id,\n",
    "                    num_beams=num_beams\n",
    "                )\n",
    "                decoded.append(decode_answer_ids(token_ids))\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d7eb2",
   "metadata": {},
   "source": [
    "# Hàm tính loss, train, dev loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6884da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e335f2031764e81a527ef0ec77aa905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 Train Loss: 9.7828\n",
      "==> Epoch 1 Dev Loss: 4.4111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7061c7d9e64656a800f058258b7110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 Train Loss: 4.3671\n",
      "==> Epoch 2 Dev Loss: 4.0187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518f84d3e20e4b55a9219a7e249def0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 Train Loss: 4.0463\n",
      "==> Epoch 3 Dev Loss: 3.8716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1546e15cad234205b5c9a38d50951f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 Train Loss: 3.8186\n",
      "==> Epoch 4 Dev Loss: 3.7417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4869f9fb5414f4ebae5a1f626e73d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 Train Loss: 3.6443\n",
      "==> Epoch 5 Dev Loss: 3.6878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7ebb30b1044e0cb6f2e45e181e487b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/3855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VQAModel(cfg, vocab_size=len(answer_itos)).to(cfg.DEVICE)\n",
    "\n",
    "# Freeze encoder (ViT + PhoBERT)\n",
    "for p in model.vit.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.vit.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "decoder_params = [p for n, p in model.named_parameters()\n",
    "                  if not n.startswith(\"vit.\") and not n.startswith(\"text_encoder.\")]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    decoder_params,\n",
    "    lr=cfg.LR_DECODER,\n",
    "    weight_decay=cfg.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "def compute_loss(logits, targets, smoothing=0.1):\n",
    "    B, Lm1, V = logits.size()\n",
    "    logits = logits.reshape(-1, V)        # (N, V)\n",
    "    targets = targets.reshape(-1)         # (N,)\n",
    "\n",
    "    # bỏ pad\n",
    "    mask = (targets != PAD_ID)\n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    logits = logits[mask]\n",
    "    targets = targets[mask]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logits)\n",
    "        true_dist.fill_(smoothing / (V - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    loss = -(true_dist * log_probs).sum(dim=-1).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=1,        \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_epoch_losses = []\n",
    "dev_epoch_losses = []\n",
    "best_dev_loss = float(\"inf\")\n",
    "best_dev = float(\"inf\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch[\"images\"].to(cfg.DEVICE)\n",
    "        q_ids  = batch[\"q_input_ids\"].to(cfg.DEVICE)\n",
    "        q_mask = batch[\"q_attention_mask\"].to(cfg.DEVICE)\n",
    "        ans_ids= batch[\"answer_ids\"].to(cfg.DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, targets = model(images, q_ids, q_mask, ans_ids)\n",
    "        loss = compute_loss(logits, targets, smoothing=0.1)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        global_step += 1\n",
    "        train_loss_sum += loss.item()\n",
    "        num_train_batches += 1\n",
    "        avg_loss = train_loss_sum / num_train_batches   # avg theo epoch, không phải global\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"avg_loss\": avg_loss})\n",
    "    \n",
    "    # ======= Train loss epoch này =======\n",
    "    epoch_train_loss = train_loss_sum / max(1, num_train_batches)\n",
    "    train_epoch_losses.append(epoch_train_loss)\n",
    "    print(f\"==> Epoch {epoch+1} Train Loss: {epoch_train_loss:.4f}\")\n",
    "    \n",
    "    # ======= Dev loss (KHÔNG tính metric trong lúc train) =======\n",
    "    model.eval()\n",
    "    dev_loss_sum = 0.0\n",
    "    dev_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            images = batch[\"images\"].to(cfg.DEVICE)\n",
    "            q_ids  = batch[\"q_input_ids\"].to(cfg.DEVICE)\n",
    "            q_mask = batch[\"q_attention_mask\"].to(cfg.DEVICE)\n",
    "            ans_ids= batch[\"answer_ids\"].to(cfg.DEVICE)\n",
    "            \n",
    "            logits, targets = model(images, q_ids, q_mask, ans_ids)\n",
    "            loss = compute_loss(logits, targets)\n",
    "            dev_loss_sum += loss.item() * images.size(0)\n",
    "            dev_count += images.size(0)\n",
    "    \n",
    "    dev_loss = dev_loss_sum / dev_count\n",
    "    dev_epoch_losses.append(dev_loss)\n",
    "    print(f\"==> Epoch {epoch+1} Dev Loss: {dev_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(dev_loss)\n",
    "\n",
    "    if dev_loss < best_dev_loss - 1e-3:\n",
    "        best_dev_loss = dev_loss\n",
    "        no_imp = 0\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.OUT_DIR, \"best_model.pt\"))\n",
    "    else:\n",
    "        no_imp += 1\n",
    "        if no_imp >= cfg.EARLY_STOP_PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "    \n",
    "    if dev_loss < best_dev_loss:\n",
    "        best_dev_loss = dev_loss\n",
    "        save_path = os.path.join(cfg.OUT_DIR, \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"    ✓ Saved best model to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20667c",
   "metadata": {},
   "source": [
    "# Sau khi train xong: Load best model + tính BLEU, METEOR, ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fcb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from outputs_vqa/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f88c17a15d4fba924c488e4de93439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Dev (metrics):   0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEV METRICS ===\n",
      "BLEU-1: 0.3057\n",
      "BLEU-2: 0.2383\n",
      "BLEU-3: 0.1922\n",
      "BLEU-4: 0.1590\n",
      "METEOR: 0.3151\n",
      "ROUGE-L: 0.4377\n"
     ]
    }
   ],
   "source": [
    "# Load lại best model (phòng trường hợp bạn chạy nhiều cell)\n",
    "best_model_path = os.path.join(cfg.OUT_DIR, \"best_model.pt\")\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=cfg.DEVICE))\n",
    "    print(f\"Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(\"Best model not found, dùng model hiện tại.\")\n",
    "\n",
    "def compute_text_metrics(refs: List[str], hyps: List[str]) -> Dict[str, float]:\n",
    "    assert len(refs) == len(hyps)\n",
    "    n = len(refs)\n",
    "    \n",
    "    bleu1 = bleu2 = bleu3 = bleu4 = 0.0\n",
    "    meteor = 0.0\n",
    "    rougeL = 0.0\n",
    "    \n",
    "    for r, h in zip(refs, hyps):\n",
    "        ref_tokens = vi_seg(r)\n",
    "        hyp_tokens = vi_seg(h)\n",
    "        \n",
    "        if len(hyp_tokens) == 0:\n",
    "            continue\n",
    "        \n",
    "        bleu1 += sentence_bleu([ref_tokens], hyp_tokens, weights=(1,0,0,0), smoothing_function=smooth_fn)\n",
    "        bleu2 += sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5,0.5,0,0), smoothing_function=smooth_fn)\n",
    "        bleu3 += sentence_bleu([ref_tokens], hyp_tokens, weights=(1/3,1/3,1/3,0), smoothing_function=smooth_fn)\n",
    "        bleu4 += sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth_fn)\n",
    "        \n",
    "        meteor += meteor_score([ref_tokens], hyp_tokens)\n",
    "        \n",
    "        rouge = rouge_scorer_obj.score(\" \".join(ref_tokens), \" \".join(hyp_tokens))[\"rougeL\"].fmeasure\n",
    "        rougeL += rouge\n",
    "    \n",
    "    # trung bình\n",
    "    return {\n",
    "        \"BLEU-1\": bleu1 / n,\n",
    "        \"BLEU-2\": bleu2 / n,\n",
    "        \"BLEU-3\": bleu3 / n,\n",
    "        \"BLEU-4\": bleu4 / n,\n",
    "        \"METEOR\": meteor / n,\n",
    "        \"ROUGE-L\": rougeL / n\n",
    "    }\n",
    "\n",
    "# Sinh dự đoán trên Dev\n",
    "model.eval()\n",
    "dev_refs = []\n",
    "dev_hyps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc=\"Eval Dev (metrics)\"):\n",
    "        images = batch[\"images\"].to(cfg.DEVICE)\n",
    "        q_ids  = batch[\"q_input_ids\"].to(cfg.DEVICE)\n",
    "        q_mask = batch[\"q_attention_mask\"].to(cfg.DEVICE)\n",
    "        \n",
    "        preds = model.generate_beam(\n",
    "            images, q_ids, q_mask,\n",
    "            max_len=cfg.MAX_ANSWER_LEN,\n",
    "            num_beams=cfg.NUM_BEAMS  \n",
    "        )\n",
    "\n",
    "        dev_hyps.extend(preds)\n",
    "        dev_refs.extend(batch[\"answers\"])\n",
    "\n",
    "metrics = compute_text_metrics(dev_refs, dev_hyps)\n",
    "print(\"=== DEV METRICS ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
